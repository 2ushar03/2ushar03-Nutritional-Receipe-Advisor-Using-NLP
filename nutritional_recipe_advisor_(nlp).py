# -*- coding: utf-8 -*-
"""Nutritional Recipe Advisor (NLP).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qTuYBNF2UBfRGKbrM1kBYKtzKfe22nGS
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df=pd.read_csv("Indain_Food_Cuisine_Dataset.csv")
df.head()

missing_values = df.isnull().sum()
print(missing_values)

print(len(df))

df.drop(['Course_name','Similar_Dishes','Prepration_time','Cooking_time','Total_time','Makes'],axis=1,inplace=True)

df.head()

from ast import literal_eval

def impute(column):
  column=column[0]
  if(type(column)!=list):
    return "".join(literal_eval(column))
  else:
    return column
  df["Ingredients_of_Dish"]=df[["Ingredients_of_Dish"]].apply(impute,axis=1)
  df["name_of_Dish"]=df[["name_of_Dish"]].apply(impute,axis=1)

df.head()

df['Recipe_Instructions']=df['Recipe_Instructions'].str.lower()
df['Ingredients_of_Dish']=df['Ingredients_of_Dish'].str.lower()
df['Diet_Type']=df['Diet_Type'].str.lower()

df.head()

import nltk
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

def recommend_food(df):
    diet_type = input("Enter the diet type (e.g., vegetarian, vegan, etc.): ").lower().strip()
    ingredients = input("Enter the ingredients you have (comma-separated): ").lower().strip()
    ingredients = ingredients.lower()
    tokens = word_tokenize(ingredients)
    stop_words = set(stopwords.words('english'))
    lemm = WordNetLemmatizer()
    filtered = [lemm.lemmatize(word) for word in tokens if word not in stop_words]
    filtered_set = set(filtered)

    Diet = df[df['Diet_Type'].str.lower().str.contains(diet_type.lower())].copy()

    if Diet.empty:
        print(f"No dishes found for diet type: {diet_type}")
        return pd.DataFrame(columns=["name_of_Dish", "Recipe_Instructions", "Ratings_of_Dish"])

    similarities = []
    for i in range(Diet.shape[0]):
        temp_tokens = word_tokenize(Diet['Ingredients_of_Dish'].iloc[i].lower())
        temp_filtered = [lemm.lemmatize(word) for word in temp_tokens if word not in stop_words]
        temp_set = set(temp_filtered)
        similarity = len(filtered_set.intersection(temp_set))
        similarities.append(similarity)

    Diet.loc[:, 'similarity'] = similarities

    Diet = Diet.sort_values(by=['similarity', 'Ratings_of_Dish'], ascending=[False, False])
    Diet.drop_duplicates(subset='name_of_Dish', keep='first', inplace=True)
    Diet = Diet.head(5).sort_values(by='Ratings_of_Dish', ascending=False)
    Diet.reset_index(drop=True, inplace=True)

    recommendations = Diet[["name_of_Dish", "Recipe_Instructions", "Ratings_of_Dish"]]

    return recommendations

recommend_food(df)